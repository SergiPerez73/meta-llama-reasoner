{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta llama rational steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLlamaRationalSteps:\n",
    "\n",
    "    def __init__(self, max_tokens=1200):\n",
    "        CONFIG = dotenv_values(\"config/.env\")\n",
    "\n",
    "        self.client = Groq(api_key=CONFIG[\"GROQ_API_KEY\"])\n",
    "        self.model_name = CONFIG[\"MODEL_NAME\"]\n",
    "        self.max_tokens = max_tokens\n",
    "    \n",
    "    def question_steps_answer(self, question, system_prompt=\"Answer the following question\"):\n",
    "        response = self.client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"{system_prompt}\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"{question}\"\n",
    "                }\n",
    "            ],\n",
    "            model=self.model_name,\n",
    "            max_tokens=self.max_tokens\n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "\n",
    "        try:\n",
    "            rational_step = answer.split(\" </think>\")[0].replace(\"<think> \", \"\")\n",
    "            final_answer = answer.split(\" </think>\")[1].replace(\"<answer> \", \"\").replace(\" </answer>\", \"\").replace(\"\\n\", \"\")\n",
    "        except:\n",
    "            rational_step = \"format error\"\n",
    "            final_answer = \"format error\"\n",
    "\n",
    "        return rational_step, final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rational_steps = MetaLlamaRationalSteps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"Find a word that relates the word 'novel' and the word 'hotel'\"\n",
    "rational_step = model_rational_steps.question_steps_answer(question=question, system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRewardPrediction(nn.Module):\n",
    "    def __init__(self, layer_config_embedding, layer_config_general, n_embeddings=2):\n",
    "        super(ModelRewardPrediction, self).__init__()\n",
    "        \n",
    "        # Create embedding layers\n",
    "        self.embedding_layers = nn.ModuleList()\n",
    "        for _ in range(n_embeddings):\n",
    "            layers = []\n",
    "            input_dim = 768\n",
    "            layer_dims = [int(dim) for dim in layer_config_embedding.split()]\n",
    "            \n",
    "            for dim in layer_dims:\n",
    "                layers.append(nn.Linear(input_dim, dim))\n",
    "                layers.append(nn.ReLU())\n",
    "                input_dim = dim\n",
    "            \n",
    "            layers.pop()  # Remove the last ReLU\n",
    "            self.embedding_layers.append(nn.Sequential(*layers))\n",
    "        \n",
    "        # Create general layers\n",
    "        layers = []\n",
    "        input_dim = n_embeddings * layer_dims[-1]\n",
    "        layer_dims = [int(dim) for dim in layer_config_general.split()]\n",
    "        \n",
    "        for dim in layer_dims:\n",
    "            layers.append(nn.Linear(input_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = dim\n",
    "        \n",
    "        layers.pop()  # Remove the last ReLU\n",
    "        layers.append(nn.Sigmoid())  # Add Sigmoid for the final layer\n",
    "        \n",
    "        self.general_layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_list):\n",
    "        # Apply embedding layers to each input\n",
    "        x_output_list = []\n",
    "        for i, x in enumerate(x_list):\n",
    "            x_output_list.append(self.embedding_layers[i](x))\n",
    "        \n",
    "        # Concatenate the outputs\n",
    "        x = torch.cat(x_output_list, dim=1)\n",
    "        \n",
    "        # Apply general layers\n",
    "        return self.general_layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5088]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_config_embedding = \"512 256\"\n",
    "layer_config_general = \"256 128 1\"\n",
    "model = ModelRewardPrediction(layer_config_embedding, layer_config_general)\n",
    "x1 = torch.randn(1, 768)\n",
    "x2 = torch.randn(1, 768)\n",
    "output = model([x1, x2])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embeddings = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "def get_embedding(sentence):\n",
    "    embedding = model_embeddings.encode(sentence)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIME_Dataset = pd.read_csv(\"data/AIME_Dataset_1983_2024.csv\")\n",
    "AIME_Dataset = AIME_Dataset[['Question', 'Answer']]\n",
    "AIME_Dataset_clarification_prompt = \" The Answer must only contain a nummber, not an explanation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_config_embedding = \"512 256\"\n",
    "layer_config_general = \"256 128 1\"\n",
    "model_reward_accuracy = ModelRewardPrediction(layer_config_embedding, layer_config_general, n_embeddings=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Let $x$ , $y$ and $z$ all exceed $1$ and let $w$ be a positive number such that $\\log_xw=24$ , $\\log_y w = 40$ and $\\log_{xyz}w=12$ . Find $\\log_zw$ .\n",
      "Answer: 60\n",
      "Model Answer: 60\n",
      "Score: tensor([[1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    question = AIME_Dataset.iloc[i]['Question']\n",
    "    answer = AIME_Dataset.iloc[i]['Answer']\n",
    "\n",
    "    rational_step, model_answer = model_rational_steps.question_steps_answer(question=question, system_prompt=system_prompt+AIME_Dataset_clarification_prompt)\n",
    "\n",
    "    model_answer = model_answer.replace(\" \", \"\")\n",
    "\n",
    "    score = 0\n",
    "    if model_answer == answer:\n",
    "        score += 1\n",
    "    \n",
    "    score = torch.tensor(score).float().view(1, 1)\n",
    "    question_embedding = torch.tensor(get_embedding(question)).unsqueeze(0)\n",
    "    rational_step_embedding = torch.tensor(get_embedding(rational_step)).unsqueeze(0)\n",
    "    model_answer_embedding = torch.tensor(get_embedding(model_answer)).unsqueeze(0)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f\"Model Answer: {model_answer}\")\n",
    "    print(f\"Score: {score}\")\n",
    "    print(\"\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_reward_accuracy([question_embedding, rational_step_embedding])\n",
    "    loss = criterion(outputs, score)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtain a df with the results on every step\n",
    "* Pass all to actual code\n",
    "* Comment code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groq2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
